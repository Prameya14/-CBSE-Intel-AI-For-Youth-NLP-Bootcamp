{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Chatbot with Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've discovered how to build a chatbot with cosine similarity. Now, let's explore how we might build one with neural network!\n",
    "\n",
    "We will create our training data, train a neural network with them, then use the trained model to make our chatbot. \n",
    "\n",
    "First, we will install required libraries. Uncomment the few blocks below only if you do not have the libraries installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy scipy\n",
    "#!pip install scikit-learn\n",
    "#!pip install pillow\n",
    "#!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Libraries\n",
    "\n",
    "Firstly, we will install libraries needed for this neural network powered chatbot. \n",
    "Keras is a machine learning library which utilizes tensorflow (another lower level machine learning library) at the backend. This makes it easier for us to deploy deep neural network for this purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Dense\n",
    " \n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input training data\n",
    "\n",
    "We will first include the following training data for our chatbot:\n",
    "1. X represent the different possible inputs that users might enter\n",
    "2. Y represent the intent of the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Hi',\n",
    "     'Hello',\n",
    "     'How are you?',\n",
    "     'I am making',\n",
    "     'making',\n",
    "     'working',\n",
    "     'studying',\n",
    "     'see you later',\n",
    "     'bye',\n",
    "     'goodbye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ['greeting',\n",
    "     'greeting',\n",
    "     'greeting',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'busy',\n",
    "     'bye',\n",
    "     'bye',\n",
    "     'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are several different sentences that have similar intent. Here, we are only having 3 intents, but you can add as many as you want for your project!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way our chatbot will work:\n",
    "1. From the input sentence, we will identify the intent using our trained AI model.\n",
    "2. For each intent, we have a prepared response. \n",
    "\n",
    "For example, if we identify that the intent of the input is for a greeting, we might ask the chatbot to reply with a greeting as well, something like 'hi' or 'how are you doing?'\n",
    "\n",
    "We will use machine learning to create a model that can classify input sentence into different intents. \n",
    "We make it as follows:\n",
    "\n",
    "1. We create a training data (X and Y above) which contains a list of sentences and their intents.\n",
    "2. Use the training data to train a classifier. \n",
    "3. Vectorize input sentences and use classifier to determine intent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text processing\n",
    "\n",
    "As usual, we will start with text processing. Do you remember the process?\n",
    "\n",
    "## 3.1 Remove non alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alpha_numeric_characters(sentence):\n",
    "    new_sentence = ''\n",
    "    for alphabet in sentence:\n",
    "        if alphabet.isalpha() or alphabet == ' ':\n",
    "            new_sentence += alphabet\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    X = [data_point.lower() for data_point in X]\n",
    "    X = [remove_non_alpha_numeric_characters(\n",
    "        sentence) for sentence in X]\n",
    "    X = [data_point.strip() for data_point in X]\n",
    "    X = [re.sub(' +', ' ',\n",
    "                data_point) for data_point in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_data(X)\n",
    "\n",
    "vocabulary = set()\n",
    "for data_point in X:\n",
    "    for word in data_point.split(' '):\n",
    "        vocabulary.add(word)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = []\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    sentence_encoded = [0] * len(vocabulary)\n",
    "    for i in range(len(vocabulary)):\n",
    "        if vocabulary[i] in sentence.split(' '):\n",
    "            sentence_encoded[i] = 1\n",
    "    return sentence_encoded\n",
    "\n",
    "X_encoded = [encode_sentence(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = list(set(Y))\n",
    "\n",
    "Y_encoded = []\n",
    "for data_point in Y:\n",
    "    data_point_encoded = [0] * len(classes)\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] == data_point:\n",
    "            data_point_encoded[i] = 1\n",
    "    Y_encoded.append(data_point_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_encoded\n",
    "y_train = Y_encoded\n",
    "X_test = X_encoded\n",
    "y_test = Y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and check the data you are using for training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [1, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 0, 1],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0],\n",
       " [0, 1, 0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does y_train represent? Do you understand the array shown above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the training data to train our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prame\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 244ms/step - loss: 1.1349\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1252\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.1138\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.1026\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0934\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0868\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0832\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0818\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0819\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0824\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0827\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0822\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0808\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0786\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0758\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0728\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0699\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0672\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0649\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0628\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0610\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0594\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0578\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0561\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0543\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0524\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0505\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0484\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0464\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.0443\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0423\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0403\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0383\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0364\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0345\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0325\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0306\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0287\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0267\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0247\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0227\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0208\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0188\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0168\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0148\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0128\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0109\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0089\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0069\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0049\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0029\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0009\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9989\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9969\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9949\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9929\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9909\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9889\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9869\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9849\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9829\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.9809\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9788\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9768\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9748\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9728\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9707\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9687\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9666\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9646\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9625\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9605\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9584\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9564\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9543\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.9522\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9501\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9480\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9460\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9439\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9418\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9397\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9376\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9354\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9333\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9312\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9291\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9269\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 973us/step - loss: 0.9248\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9227\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.9205\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9184\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 973us/step - loss: 0.9162\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9140\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9119\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9097\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9075\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9053\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9031\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27feee39810>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid',\n",
    "                input_dim=len(X_train[0])))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01,\n",
    "                            momentum=0.9, nesterov=True))\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List down predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = [argmax(pred) for pred in model.predict(np.array(X_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate our model now. We will compare the prediction made by the model and our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 6\n",
      "Total: 10\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == argmax(y_test[i]):\n",
    "        correct += 1\n",
    "\n",
    "print (\"Correct:\", correct)\n",
    "print (\"Total:\", len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the chatbot now! We will input a sentence, and then see what class is predicted by the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print (\"Enter a sentence\")\n",
    "    sentence = input()\n",
    "    prediction= model.predict(np.array([encode_sentence(sentence)]))\n",
    "    print (classes[argmax(prediction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize that you can't stop the chatbot? You'll have to add the exit command later (see the previous notebook to find out how to do it. \n",
    "\n",
    "For now, simply press the stop button (interrupt button) above to stop the chatbot. \n",
    "\n",
    "Try it! press the stop button, and try typing something onto the box. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "We have successfully use neural network to map our input to conversation intent. \n",
    "Your challenge is to link the conversation intent to a particular response that the chatbot will say. \n",
    "For example, if the conversation intent is 'greeting', get your chatbot to say a greeting as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "busy\n",
      "Enter a sentence\n"
     ]
    }
   ],
   "source": [
    "working = True\n",
    "while working:\n",
    "  print (\"Enter a sentence\")\n",
    "  sentence = input()\n",
    "  if 'bye' not in sentence.lower():\n",
    "    prediction= model.predict(np.array([encode_sentence(sentence)]))\n",
    "    print(classes[argmax(prediction)])\n",
    "  else:\n",
    "    working = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great job! You've successfully created a simple chatbot with neural network! How might you improve the chatbot?\n",
    "You can improve the chatbot by:\n",
    "- Adding more training data\n",
    "- Adding more intent\n",
    "- Focusing on a particular topic and train the chatbot with many training data in that topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource:\n",
    "https://blog.eduonix.com/internet-of-things/simple-nlp-based-chatbot-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d09ebe33cb5488305776998fc9bd4009441ccc193fca551d3078796f3e290d82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
